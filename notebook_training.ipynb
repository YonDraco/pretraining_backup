{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4df4458a-c018-43a7-8668-f97ef5b076b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 28 03:44:53 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           Off | 00000000:03:00.0 Off |                    0 |\n",
      "| N/A   36C    P0              39W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80d33277-365f-4069-8263-22092a538863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/anhhn/training_qwen\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d145ad1e-4fb2-4edb-a5be-a58f7ab9bce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/anhhn/env/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/anhhn/env/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/anhhn/env/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/anhhn/env/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/anhhn/env/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3452b378-4527-424a-a8fd-d78cd688ac52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in /home/anhhn/env/lib/python3.10/site-packages (0.3.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/anhhn/env/lib/python3.10/site-packages (from llama-cpp-python) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/anhhn/env/lib/python3.10/site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /home/anhhn/env/lib/python3.10/site-packages (from llama-cpp-python) (3.1.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /home/anhhn/env/lib/python3.10/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/anhhn/env/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc3dbc8-12e4-47d7-abb5-599c3fc3b616",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c65a52-d9fe-49a0-a756-4c95bf187b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.11.7: Fast Qwen2 patching. Transformers = 4.46.1.\n",
      "   \\\\   /|    GPU: Tesla V100-SXM2-32GB. Max memory: 31.739 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1+cu124. CUDA = 7.0. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]^C\n"
     ]
    }
   ],
   "source": [
    "!python train_v1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d425cfd6-1c6a-4253-8a1d-97f8d2837b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.sampling_params import GuidedDecodingParams\n",
    "from dataset import Prompter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "prompter = Prompter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02b35979",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-20 10:07:22 config.py:1861] Downcasting torch.float32 to torch.float16.\n",
      "INFO 11-20 10:07:27 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.\n",
      "WARNING 11-20 10:07:27 config.py:428] gguf quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 11-20 10:07:27 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='checkpoint/unsloth.Q5_K_M.gguf', speculative_config=None, tokenizer='checkpoint/unsloth.Q5_K_M.gguf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.GGUF, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gguf, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=checkpoint/unsloth.Q5_K_M.gguf, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "INFO 11-20 10:08:05 selector.py:261] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 11-20 10:08:05 selector.py:144] Using XFormers backend.\n",
      "INFO 11-20 10:08:06 model_runner.py:1072] Starting to load model checkpoint/unsloth.Q5_K_M.gguf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anhhn/training_qwen/venv/lib/python3.10/site-packages/torch/nested/__init__.py:226: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  return _nested.nested_tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-20 10:09:44 model_runner.py:1077] Loading model weights took 9.8409 GB\n",
      "INFO 11-20 10:10:03 worker.py:232] Memory profiling results: total_gpu_memory=31.74GiB initial_memory_usage=10.38GiB peak_torch_memory=11.26GiB memory_usage_post_profile=10.40GiB non_torch_memory=0.56GiB kv_cache_size=0.87GiB gpu_memory_utilization=0.40\n",
      "INFO 11-20 10:10:04 gpu_executor.py:113] # GPU blocks: 298, # CPU blocks: 1365\n",
      "INFO 11-20 10:10:04 gpu_executor.py:117] Maximum concurrency for 4096 tokens per request: 1.16x\n",
      "INFO 11-20 10:10:07 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-20 10:10:07 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-20 10:11:07 model_runner.py:1518] Graph capturing finished in 60 secs, took 2.62 GiB\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "        model=\"checkpoint/unsloth.Q5_K_M.gguf\", max_model_len=4096, gpu_memory_utilization=0.4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcdf7c3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def run(\n",
    "    tweet: str | list[str],\n",
    "    model: LLM,\n",
    "    prompter: Prompter = prompter,\n",
    "    temperature: float = 0.1,\n",
    "    max_tokens: int = 128,\n",
    "    **kwargs\n",
    ") -> list[str]:\n",
    "\n",
    "    if isinstance(tweet, str):\n",
    "        tweet = [tweet]\n",
    "\n",
    "    messages: list[list[dict[str, str]]] = []\n",
    "    for tweet_ in tweet:\n",
    "        message = [\n",
    "            {\"role\": \"system\", \"content\": prompter.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompter.prompt_template.format(tweet=tweet_)}\n",
    "        ]\n",
    "        messages.append(message)\n",
    "    \n",
    "    print(len(messages))\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_k=kwargs.get(\"top_k\", 40),\n",
    "        top_p=kwargs.get(\"top_p\", 1.0),\n",
    "        repetition_penalty=kwargs.get(\"repetition_penalty\", 1.0),\n",
    "        guided_decoding=GuidedDecodingParams(choice=[\"calling\", \"not_calling\"])\n",
    "    )\n",
    "\n",
    "    outputs = model.chat(\n",
    "        messages=messages, \n",
    "        sampling_params=sampling_params,\n",
    "        use_tqdm=True,\n",
    "    )\n",
    "\n",
    "    outputs = [output.outputs[0].text.strip() for output in outputs]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c902bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data/data_infer_100sample.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22655b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df[\"full_text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e774e259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 32/32 [00:44<00:00,  1.38s/it, est. speed input: 226.04 toks/s, output: 2.20 toks/s]\n",
      " 25%|██▌       | 1/4 [00:44<02:12, 44.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 32/32 [00:44<00:00,  1.38s/it, est. speed input: 225.07 toks/s, output: 2.25 toks/s]\n",
      " 50%|█████     | 2/4 [01:28<01:28, 44.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 32/32 [00:45<00:00,  1.42s/it, est. speed input: 224.61 toks/s, output: 2.35 toks/s]\n",
      " 75%|███████▌  | 3/4 [02:14<00:44, 44.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it, est. speed input: 220.81 toks/s, output: 2.20 toks/s]\n",
      "100%|██████████| 4/4 [02:19<00:00, 34.95s/it]\n"
     ]
    }
   ],
   "source": [
    "predicts = []\n",
    "batch_size = 32\n",
    "for i in range(0, len(tweets), batch_size):\n",
    "    batch_tweet = tweets[i: i + batch_size]\n",
    "    batch_predict = run(tweet=batch_tweet, model=llm)\n",
    "    predicts.extend(batch_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6852b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not_calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'calling',\n",
       " 'calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'calling',\n",
       " 'calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'not_calling',\n",
       " 'calling',\n",
       " 'not_calling',\n",
       " 'calling']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e213bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97f72fc4-2437-4237-a7a9-3b6c8db04a91",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                           Version\n",
      "--------------------------------- -------------\n",
      "accelerate                        1.1.1\n",
      "aiohappyeyeballs                  2.4.3\n",
      "aiohttp                           3.11.6\n",
      "aiosignal                         1.3.1\n",
      "annotated-types                   0.7.0\n",
      "anyio                             4.6.2.post1\n",
      "appdirs                           1.4.4\n",
      "asttokens                         2.4.1\n",
      "async-timeout                     5.0.1\n",
      "attrs                             24.2.0\n",
      "bitsandbytes                      0.44.1\n",
      "certifi                           2024.8.30\n",
      "charset-normalizer                3.4.0\n",
      "click                             8.1.7\n",
      "cloudpickle                       3.1.0\n",
      "comm                              0.2.2\n",
      "compressed-tensors                0.8.0\n",
      "datasets                          3.1.0\n",
      "debugpy                           1.8.8\n",
      "decorator                         5.1.1\n",
      "dill                              0.3.8\n",
      "diskcache                         5.6.3\n",
      "distro                            1.9.0\n",
      "docker-pycreds                    0.4.0\n",
      "docstring_parser                  0.16\n",
      "einops                            0.8.0\n",
      "et_xmlfile                        2.0.0\n",
      "exceptiongroup                    1.2.2\n",
      "executing                         2.1.0\n",
      "fastapi                           0.115.5\n",
      "filelock                          3.16.1\n",
      "fire                              0.7.0\n",
      "frozenlist                        1.5.0\n",
      "fsspec                            2024.9.0\n",
      "gguf                              0.10.0\n",
      "gitdb                             4.0.11\n",
      "GitPython                         3.1.43\n",
      "h11                               0.14.0\n",
      "hf_transfer                       0.1.8\n",
      "httpcore                          1.0.7\n",
      "httptools                         0.6.4\n",
      "httpx                             0.27.2\n",
      "huggingface-hub                   0.26.2\n",
      "idna                              3.10\n",
      "importlib_metadata                8.5.0\n",
      "interegular                       0.3.3\n",
      "ipykernel                         6.29.5\n",
      "ipython                           8.29.0\n",
      "jedi                              0.19.2\n",
      "Jinja2                            3.1.4\n",
      "jiter                             0.7.1\n",
      "joblib                            1.4.2\n",
      "jsonschema                        4.23.0\n",
      "jsonschema-specifications         2024.10.1\n",
      "jupyter_client                    8.6.3\n",
      "jupyter_core                      5.7.2\n",
      "lark                              1.2.2\n",
      "llama_cpp_python                  0.3.2\n",
      "llvmlite                          0.43.0\n",
      "lm-format-enforcer                0.10.9\n",
      "markdown-it-py                    3.0.0\n",
      "MarkupSafe                        3.0.2\n",
      "matplotlib-inline                 0.1.7\n",
      "mdurl                             0.1.2\n",
      "mistral_common                    1.5.0\n",
      "mpmath                            1.3.0\n",
      "msgpack                           1.1.0\n",
      "msgspec                           0.18.6\n",
      "multidict                         6.1.0\n",
      "multiprocess                      0.70.16\n",
      "nest-asyncio                      1.6.0\n",
      "networkx                          3.4.2\n",
      "numba                             0.60.0\n",
      "numpy                             1.26.4\n",
      "nvidia-cublas-cu12                12.4.5.8\n",
      "nvidia-cuda-cupti-cu12            12.4.127\n",
      "nvidia-cuda-nvrtc-cu12            12.4.127\n",
      "nvidia-cuda-runtime-cu12          12.4.127\n",
      "nvidia-cudnn-cu12                 9.1.0.70\n",
      "nvidia-cufft-cu12                 11.2.1.3\n",
      "nvidia-curand-cu12                10.3.5.147\n",
      "nvidia-cusolver-cu12              11.6.1.9\n",
      "nvidia-cusparse-cu12              12.3.1.170\n",
      "nvidia-ml-py                      12.560.30\n",
      "nvidia-nccl-cu12                  2.21.5\n",
      "nvidia-nvjitlink-cu12             12.4.127\n",
      "nvidia-nvtx-cu12                  12.4.127\n",
      "openai                            1.54.5\n",
      "opencv-python-headless            4.10.0.84\n",
      "openpyxl                          3.1.5\n",
      "outlines                          0.0.46\n",
      "packaging                         24.2\n",
      "pandas                            2.2.3\n",
      "parso                             0.8.4\n",
      "partial-json-parser               0.2.1.1.post4\n",
      "pathtools                         0.1.2\n",
      "peft                              0.13.2\n",
      "pexpect                           4.9.0\n",
      "pillow                            10.4.0\n",
      "pip                               22.0.2\n",
      "platformdirs                      4.3.6\n",
      "prometheus_client                 0.21.0\n",
      "prometheus-fastapi-instrumentator 7.0.0\n",
      "prompt_toolkit                    3.0.48\n",
      "propcache                         0.2.0\n",
      "protobuf                          3.20.3\n",
      "psutil                            6.1.0\n",
      "ptyprocess                        0.7.0\n",
      "pure_eval                         0.2.3\n",
      "py-cpuinfo                        9.0.0\n",
      "pyairports                        2.1.1\n",
      "pyarrow                           18.0.0\n",
      "pycountry                         24.6.1\n",
      "pydantic                          2.9.2\n",
      "pydantic_core                     2.23.4\n",
      "Pygments                          2.18.0\n",
      "python-dateutil                   2.9.0.post0\n",
      "python-dotenv                     1.0.1\n",
      "pytz                              2024.2\n",
      "PyYAML                            6.0.2\n",
      "pyzmq                             26.2.0\n",
      "ray                               2.39.0\n",
      "referencing                       0.35.1\n",
      "regex                             2024.11.6\n",
      "requests                          2.32.3\n",
      "rich                              13.9.4\n",
      "rpds-py                           0.21.0\n",
      "safetensors                       0.4.5\n",
      "scikit-learn                      1.5.2\n",
      "scipy                             1.14.1\n",
      "sentencepiece                     0.2.0\n",
      "sentry-sdk                        2.17.0\n",
      "setproctitle                      1.3.4\n",
      "setuptools                        59.6.0\n",
      "shtab                             1.7.1\n",
      "six                               1.16.0\n",
      "smmap                             5.0.1\n",
      "sniffio                           1.3.1\n",
      "stack-data                        0.6.3\n",
      "starlette                         0.41.3\n",
      "sympy                             1.13.1\n",
      "termcolor                         2.5.0\n",
      "threadpoolctl                     3.5.0\n",
      "tiktoken                          0.7.0\n",
      "tokenizers                        0.20.3\n",
      "torch                             2.5.1\n",
      "torchvision                       0.20.1\n",
      "tornado                           6.4.1\n",
      "tqdm                              4.67.0\n",
      "traitlets                         5.14.3\n",
      "transformers                      4.46.1\n",
      "triton                            3.1.0\n",
      "trl                               0.12.1\n",
      "typing_extensions                 4.12.2\n",
      "tyro                              0.9.1\n",
      "tzdata                            2024.2\n",
      "unsloth                           2024.11.7\n",
      "unsloth_zoo                       2024.11.5\n",
      "urllib3                           2.2.3\n",
      "uvicorn                           0.32.0\n",
      "uvloop                            0.21.0\n",
      "vllm                              0.6.4.post1\n",
      "wandb                             0.15.8\n",
      "watchfiles                        0.24.0\n",
      "wcwidth                           0.2.13\n",
      "websockets                        14.1\n",
      "wheel                             0.45.0\n",
      "xformers                          0.0.28.post3\n",
      "xxhash                            3.5.0\n",
      "yarl                              1.17.2\n",
      "zipp                              3.21.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53241bbd-5407-42bc-99d8-30cdfb1d214f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|                                 | 0/8 [00:00<?, ?it/s]\n",
      "model-00001-of-00008.safetensors:   0%|            | 10.5M/3.89G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   1%|   | 21.0M/3.89G [00:21<2:14:10, 480kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   1%|   | 21.0M/3.89G [00:39<2:14:10, 480kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   1%|   | 31.5M/3.89G [00:44<2:16:05, 472kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   1%|   | 31.5M/3.89G [00:59<2:16:05, 472kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   1%|   | 41.9M/3.89G [01:05<2:11:52, 486kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   1%|   | 41.9M/3.89G [01:19<2:11:52, 486kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   1%|   | 52.4M/3.89G [01:28<2:16:20, 469kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   1%|   | 52.4M/3.89G [01:39<2:16:20, 469kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   2%|   | 62.9M/3.89G [01:53<2:21:18, 451kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   2%|   | 62.9M/3.89G [02:09<2:21:18, 451kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   2%|   | 73.4M/3.89G [02:17<2:22:12, 447kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   2%|   | 73.4M/3.89G [02:29<2:22:12, 447kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   2%|   | 83.9M/3.89G [02:42<2:24:20, 439kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   2%|   | 83.9M/3.89G [02:59<2:24:20, 439kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   2%|   | 94.4M/3.89G [03:05<2:22:04, 445kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   2%|   | 94.4M/3.89G [03:19<2:22:04, 445kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   3%|    | 105M/3.89G [03:30<2:24:39, 436kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   3%|    | 105M/3.89G [03:49<2:24:39, 436kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   3%|    | 115M/3.89G [03:52<2:20:32, 447kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   3%|    | 115M/3.89G [04:09<2:20:32, 447kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   3%|▏   | 126M/3.89G [04:15<2:19:24, 449kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   3%|▏   | 126M/3.89G [04:29<2:19:24, 449kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   4%|▏   | 136M/3.89G [04:43<2:28:09, 422kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   4%|▏   | 136M/3.89G [04:59<2:28:09, 422kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   4%|▏   | 147M/3.89G [05:11<2:31:59, 410kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   4%|▏   | 147M/3.89G [05:29<2:31:59, 410kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   4%|▏   | 157M/3.89G [05:32<2:24:18, 431kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   4%|▏   | 157M/3.89G [05:49<2:24:18, 431kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   4%|▏   | 168M/3.89G [05:57<2:24:32, 429kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   4%|▏   | 168M/3.89G [06:09<2:24:32, 429kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   5%|▏   | 178M/3.89G [06:24<2:28:52, 415kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   5%|▏   | 178M/3.89G [06:39<2:28:52, 415kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   5%|▏   | 189M/3.89G [06:50<2:30:28, 409kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   5%|▏   | 189M/3.89G [07:09<2:30:28, 409kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   5%|▏   | 199M/3.89G [07:11<2:21:55, 433kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   5%|▏   | 199M/3.89G [07:29<2:21:55, 433kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   5%|▏   | 210M/3.89G [07:34<2:18:07, 443kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   5%|▏   | 210M/3.89G [07:49<2:18:07, 443kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   6%|▏   | 220M/3.89G [08:00<2:22:16, 429kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   6%|▏   | 220M/3.89G [08:19<2:22:16, 429kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   6%|▏   | 231M/3.89G [08:42<2:53:35, 351kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   6%|▏   | 231M/3.89G [08:59<2:53:35, 351kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   6%|▏   | 241M/3.89G [09:02<2:35:56, 389kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   6%|▏   | 241M/3.89G [09:19<2:35:56, 389kB/s]\u001b[A\n",
      "model-00001-of-00008.safetensors:   6%|▎   | 252M/3.89G [09:22<2:22:52, 424kB/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "!python test_pretraining.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f64beb-73b2-447c-8fa9-96f8ff6a91d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78044fe9-2726-414f-b1ac-54fda8968ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182e64ea-8aa9-4435-88f6-b44db825e6cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
